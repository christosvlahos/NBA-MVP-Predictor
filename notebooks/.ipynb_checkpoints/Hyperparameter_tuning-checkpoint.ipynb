{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4da424-e912-416b-a304-e2411f9c3b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 17:25:43.977322: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "\n",
    "from Model_functions import *\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#from sklearn.metrics import r2_score\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.simplefilter(action = 'ignore', category=SettingWithCopyWarning)\n",
    "pd.set_option('display.max_columns', None, 'display.max.rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d76de67c-55b4-4e38-951d-54f9f13bc7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_game_data = pd.read_csv(\"data/MVP_per_game.csv\")\n",
    "#per_possession_data = pd.read_csv(\"data/MVP_per_possession.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ecbc109-82be-4c2e-86e1-af4de0f496cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = 2022\n",
    "years = [year for year in range(1980, 2022)]\n",
    "\n",
    "historical_data = per_game_data[per_game_data[\"Year\"] != test_year]\n",
    "final_year_data = per_game_data[per_game_data[\"Year\"] == test_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166b6a9e-3a6e-4122-a523-fe95afc65273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of important features extracted in MVP_predictions.ipynb\n",
    "\n",
    "feats = ['PER', 'WS/48', 'VORP', 'BPM', 'OBPM', 'OWS', 'PTS', 'USG%',\n",
    "       'W/L%', 'TS%', 'TOV', 'DWS', 'FTr', 'DRB%']\n",
    "\n",
    "id_cols = ['Rank','Player', 'Age', 'Tm', 'Year', 'Share']\n",
    "\n",
    "historical_data_reduced = historical_data[id_cols + feats]\n",
    "final_year_data_reduced = final_year_data[feats]\n",
    "\n",
    "id_train = historical_data[['Rank','Player', 'Age', 'Tm', 'Year']]\n",
    "id_test = final_year_data[['Rank','Player', 'Age', 'Tm', 'Year']]\n",
    "\n",
    "y_train = historical_data['Share']\n",
    "y_test = final_year_data['Share']\n",
    "\n",
    "x_train = historical_data[feats]\n",
    "x_test= final_year_data[feats]\n",
    "\n",
    "#to_drop_train = ['Rank','Player', 'Age', 'Tm', 'Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ace6c-e545-4d2f-8163-0e0e26a6b97a",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d17370e-6fd3-4362-a2bd-44d14ed27a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_sNN = {#'epochs': [50,70],\n",
    "#               'n1': [20,30],\n",
    "#               'n2': [20,30],\n",
    "#               #'batch_size':[50, 70],\n",
    "#               'dropout': [0.1, 0.2, 0.3],\n",
    "#               'optimizer': [keras.optimizers.SGD] ,\n",
    "#               'learn_rate': [0.15,0.2]#,\n",
    "#               #'model__momentum': [0.0, 0.3, 0.7]\n",
    "#              }\n",
    "\n",
    "# model_sNN = KerasRegressor(keras_model,\n",
    "#                            learn_rate = param_sNN[\"learn_rate\"],\n",
    "#                            optimizer = param_sNN[\"optimizer\"],\n",
    "#                            dropout = param_sNN[\"dropout\"],\n",
    "#                            #epochs = param_sNN[\"epochs\"],\n",
    "#                            n1 = param_sNN[\"n1\"],\n",
    "#                            n2 = param_sNN[\"n2\"],\n",
    "#                            #batch_size = param_sNN[\"batch_size\"]\n",
    "#                           )\n",
    "\n",
    "# estimator = Pipeline([('scaler', StandardScaler()), \n",
    "#                       (\"model\", model_sNN)])\n",
    "\n",
    "\n",
    "# grid_sNN = GridSearchCV(estimator=estimator, param_grid=param_sNN, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5947fce1-632b-41bf-87dd-36085f22b325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_d/d5z6h7kx6ws38vzmkgr9pss40000gp/T/ipykernel_743/1983472644.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model_NN = KerasRegressor(build_fn=keras_model, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "model_NN = KerasRegressor(build_fn=keras_model, verbose=0)\n",
    "\n",
    "estimator = Pipeline([('scaler', StandardScaler()), \n",
    "                      (\"model\", model_NN)])\n",
    "\n",
    "param_grid = {'model__epochs': [50,70],\n",
    "              'model__n1': [20,30],\n",
    "              'model__n2': [20,30],\n",
    "              'model__batch_size':[50, 70],\n",
    "              'model__dropout': [0.1, 0.2, 0.3],\n",
    "              'model__optimizer': [keras.optimizers.SGD, keras.optimizers.Adam] ,\n",
    "              'model__learn_rate': [0.15,0.2]#,\n",
    "              #'model__momentum': [0.0, 0.3, 0.7]\n",
    "             }\n",
    "\n",
    "grid_NN = GridSearchCV(estimator=estimator, param_grid=param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1361152-59e2-4e52-ac66-b3dcec6d8dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 16:02:40.238512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 16:02:40.243934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 16:02:40.257848: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 16:02:40.262679: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 16:02:51.917635: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 16:02:51.926628: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 16:02:51.936233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 16:02:51.978251: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x1274ebc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x12bb8ddc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x126bb0c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x12ac59dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x12767ee50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x12bb8df70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x126d44e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x12ac59f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2022-09-09 16:20:12.496980: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        &lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;model__batch_size&#x27;: [50, 70],\n",
       "                         &#x27;model__dropout&#x27;: [0.1, 0.2, 0.3],\n",
       "                         &#x27;model__epochs&#x27;: [50, 70],\n",
       "                         &#x27;model__learn_rate&#x27;: [0.15, 0.2],\n",
       "                         &#x27;model__n1&#x27;: [20, 30], &#x27;model__n2&#x27;: [20, 30],\n",
       "                         &#x27;model__optimizer&#x27;: [&lt;class &#x27;keras.optimizers.optimizer_v2.gradient_descent.SGD&#x27;&gt;,\n",
       "                                              &lt;class &#x27;keras.optimizers.optimizer_v2.adam.Adam&#x27;&gt;]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        &lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;model__batch_size&#x27;: [50, 70],\n",
       "                         &#x27;model__dropout&#x27;: [0.1, 0.2, 0.3],\n",
       "                         &#x27;model__epochs&#x27;: [50, 70],\n",
       "                         &#x27;model__learn_rate&#x27;: [0.15, 0.2],\n",
       "                         &#x27;model__n1&#x27;: [20, 30], &#x27;model__n2&#x27;: [20, 30],\n",
       "                         &#x27;model__optimizer&#x27;: [&lt;class &#x27;keras.optimizers.optimizer_v2.gradient_descent.SGD&#x27;&gt;,\n",
       "                                              &lt;class &#x27;keras.optimizers.optimizer_v2.adam.Adam&#x27;&gt;]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 &lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('model',\n",
       "                                        <keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00>)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'model__batch_size': [50, 70],\n",
       "                         'model__dropout': [0.1, 0.2, 0.3],\n",
       "                         'model__epochs': [50, 70],\n",
       "                         'model__learn_rate': [0.15, 0.2],\n",
       "                         'model__n1': [20, 30], 'model__n2': [20, 30],\n",
       "                         'model__optimizer': [<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>,\n",
       "                                              <class 'keras.optimizers.optimizer_v2.adam.Adam'>]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_NN.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1291f037-04e7-4331-9ab2-d1146ebe2512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__batch_size': 50,\n",
       " 'model__dropout': 0.2,\n",
       " 'model__epochs': 70,\n",
       " 'model__learn_rate': 0.15,\n",
       " 'model__n1': 30,\n",
       " 'model__n2': 30,\n",
       " 'model__optimizer': keras.optimizers.optimizer_v2.gradient_descent.SGD}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_NN.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "394954fc-ea5b-480f-847e-54092a1c85b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid2 = {'model__epochs': [60, 70, 80],\n",
    "              'model__n1': [30,40],\n",
    "              'model__n2': [30,40],\n",
    "              'model__batch_size':[40, 50, 60],\n",
    "              'model__dropout': [0.15, 0.2, 0.25],\n",
    "              'model__optimizer': [keras.optimizers.SGD],#['RMSprop', 'Adam', 'sgd'],\n",
    "              'model__learn_rate': [0.12, 0.15, 0.18]\n",
    "             }\n",
    "\n",
    "grid_NN2 = GridSearchCV(estimator=estimator, param_grid=param_grid2, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd197b61-0586-4f7e-ad9e-d8eb8295902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        &lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;model__batch_size&#x27;: [40, 50, 60],\n",
       "                         &#x27;model__dropout&#x27;: [0.15, 0.2, 0.25],\n",
       "                         &#x27;model__epochs&#x27;: [60, 70, 80],\n",
       "                         &#x27;model__learn_rate&#x27;: [0.12, 0.15, 0.18],\n",
       "                         &#x27;model__n1&#x27;: [30, 40], &#x27;model__n2&#x27;: [30, 40],\n",
       "                         &#x27;model__optimizer&#x27;: [&lt;class &#x27;keras.optimizers.optimizer_v2.gradient_descent.SGD&#x27;&gt;]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        &lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;model__batch_size&#x27;: [40, 50, 60],\n",
       "                         &#x27;model__dropout&#x27;: [0.15, 0.2, 0.25],\n",
       "                         &#x27;model__epochs&#x27;: [60, 70, 80],\n",
       "                         &#x27;model__learn_rate&#x27;: [0.12, 0.15, 0.18],\n",
       "                         &#x27;model__n1&#x27;: [30, 40], &#x27;model__n2&#x27;: [30, 40],\n",
       "                         &#x27;model__optimizer&#x27;: [&lt;class &#x27;keras.optimizers.optimizer_v2.gradient_descent.SGD&#x27;&gt;]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 &lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('model',\n",
       "                                        <keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00>)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'model__batch_size': [40, 50, 60],\n",
       "                         'model__dropout': [0.15, 0.2, 0.25],\n",
       "                         'model__epochs': [60, 70, 80],\n",
       "                         'model__learn_rate': [0.12, 0.15, 0.18],\n",
       "                         'model__n1': [30, 40], 'model__n2': [30, 40],\n",
       "                         'model__optimizer': [<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_NN2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb054ff-324e-47f5-8f8c-e5dc9917fde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__batch_size': 40,\n",
       " 'model__dropout': 0.15,\n",
       " 'model__epochs': 70,\n",
       " 'model__learn_rate': 0.15,\n",
       " 'model__n1': 30,\n",
       " 'model__n2': 40,\n",
       " 'model__optimizer': keras.optimizers.optimizer_v2.gradient_descent.SGD}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_NN2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0512d061-657e-47a1-9bf4-fcd0089e526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid3 = {'model__epochs': [70, 100],\n",
    "              'model__n1': [30,40],\n",
    "              'model__n2': [40,50],\n",
    "              'model__batch_size':[40, 50],\n",
    "              'model__dropout': [0.15, 0.18],\n",
    "              'model__optimizer': [keras.optimizers.SGD],#['RMSprop', 'Adam', 'sgd'],\n",
    "              'model__learn_rate': [0.15]\n",
    "             }\n",
    "\n",
    "grid_NN3 = GridSearchCV(estimator=estimator, param_grid=param_grid3, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cef48179-2bf4-4aa3-be35-727898189498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 17:04:21.126811: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 17:04:21.127016: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 17:04:21.126815: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 17:04:21.127893: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 17:04:35.140618: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 17:04:35.140617: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 17:04:35.140618: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 17:04:35.140618: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x126de9ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x126757ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x126e7fb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x1229f9ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x126aaeb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x123a14ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x122e97b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x12388eb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        &lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;model__batch_size&#x27;: [40, 50],\n",
       "                         &#x27;model__dropout&#x27;: [0.15, 0.18],\n",
       "                         &#x27;model__epochs&#x27;: [70, 100],\n",
       "                         &#x27;model__learn_rate&#x27;: [0.15], &#x27;model__n1&#x27;: [30, 40],\n",
       "                         &#x27;model__n2&#x27;: [40, 50],\n",
       "                         &#x27;model__optimizer&#x27;: [&lt;class &#x27;keras.optimizers.optimizer_v2.gradient_descent.SGD&#x27;&gt;]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        &lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;model__batch_size&#x27;: [40, 50],\n",
       "                         &#x27;model__dropout&#x27;: [0.15, 0.18],\n",
       "                         &#x27;model__epochs&#x27;: [70, 100],\n",
       "                         &#x27;model__learn_rate&#x27;: [0.15], &#x27;model__n1&#x27;: [30, 40],\n",
       "                         &#x27;model__n2&#x27;: [40, 50],\n",
       "                         &#x27;model__optimizer&#x27;: [&lt;class &#x27;keras.optimizers.optimizer_v2.gradient_descent.SGD&#x27;&gt;]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 &lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('model',\n",
       "                                        <keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00>)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'model__batch_size': [40, 50],\n",
       "                         'model__dropout': [0.15, 0.18],\n",
       "                         'model__epochs': [70, 100],\n",
       "                         'model__learn_rate': [0.15], 'model__n1': [30, 40],\n",
       "                         'model__n2': [40, 50],\n",
       "                         'model__optimizer': [<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_NN3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84e02967-7167-449a-bd36-85ef7dbdaf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__batch_size': 40,\n",
       " 'model__dropout': 0.15,\n",
       " 'model__epochs': 100,\n",
       " 'model__learn_rate': 0.15,\n",
       " 'model__n1': 40,\n",
       " 'model__n2': 50,\n",
       " 'model__optimizer': keras.optimizers.optimizer_v2.gradient_descent.SGD}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_NN3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de50211b-6928-423f-b0f0-ce8cb3a6cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid4 = {'model__epochs': [80, 100],\n",
    "              'model__n1': [40,50],\n",
    "              'model__n2': [50,60],\n",
    "              'model__batch_size':[40],\n",
    "              'model__dropout': [0.15, 0.18],\n",
    "              'model__optimizer': [keras.optimizers.SGD],#['RMSprop', 'Adam', 'sgd'],\n",
    "              'model__learn_rate': [0.15]\n",
    "             }\n",
    "\n",
    "grid_NN4 = GridSearchCV(estimator=estimator, param_grid=param_grid4, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78dcae18-dbd6-4ae9-b0ca-3bea9e2f95b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        &lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;model__batch_size&#x27;: [40],\n",
       "                         &#x27;model__dropout&#x27;: [0.15, 0.18],\n",
       "                         &#x27;model__epochs&#x27;: [80, 100],\n",
       "                         &#x27;model__learn_rate&#x27;: [0.15], &#x27;model__n1&#x27;: [40, 50],\n",
       "                         &#x27;model__n2&#x27;: [50, 60],\n",
       "                         &#x27;model__optimizer&#x27;: [&lt;class &#x27;keras.optimizers.optimizer_v2.gradient_descent.SGD&#x27;&gt;]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        &lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;model__batch_size&#x27;: [40],\n",
       "                         &#x27;model__dropout&#x27;: [0.15, 0.18],\n",
       "                         &#x27;model__epochs&#x27;: [80, 100],\n",
       "                         &#x27;model__learn_rate&#x27;: [0.15], &#x27;model__n1&#x27;: [40, 50],\n",
       "                         &#x27;model__n2&#x27;: [50, 60],\n",
       "                         &#x27;model__optimizer&#x27;: [&lt;class &#x27;keras.optimizers.optimizer_v2.gradient_descent.SGD&#x27;&gt;]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 &lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00&gt;</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('model',\n",
       "                                        <keras.wrappers.scikit_learn.KerasRegressor object at 0x13421ba00>)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'model__batch_size': [40],\n",
       "                         'model__dropout': [0.15, 0.18],\n",
       "                         'model__epochs': [80, 100],\n",
       "                         'model__learn_rate': [0.15], 'model__n1': [40, 50],\n",
       "                         'model__n2': [50, 60],\n",
       "                         'model__optimizer': [<class 'keras.optimizers.optimizer_v2.gradient_descent.SGD'>]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_NN4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63f7816b-9ca8-4db9-b667-1b14d9b98498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__batch_size': 40,\n",
       " 'model__dropout': 0.15,\n",
       " 'model__epochs': 80,\n",
       " 'model__learn_rate': 0.15,\n",
       " 'model__n1': 40,\n",
       " 'model__n2': 60,\n",
       " 'model__optimizer': keras.optimizers.optimizer_v2.gradient_descent.SGD}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_NN4.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3de705-09b3-4d83-b8b3-2817823002f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameter Tuning for the Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398980e1-f852-4799-9ac5-a0d0224a0139",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Method 1: Using cross-validation in RandomizedSearchcv and GridSearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58efabc2-7c06-4fea-aec1-c503d09f40eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "{'n_estimators': 45, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_depth': 20}\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter tuning for the Random Forest model\n",
    "\n",
    "forest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "#number of trees\n",
    "n_estim = [int(x) for x in np.linspace(10, 50, num = 9)]\n",
    "# maximum number of levels allowed in each decision tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 30, num = 5)]\n",
    "# minimum sample number to split a node\n",
    "min_samples_split = [2, 6, 8]\n",
    "# minimum sample number that can be stored in a leaf node\n",
    "min_samples_leaf = [1, 3, 4, 7]\n",
    "# method used to sample data points. All initial random searches select True\n",
    "#bootstrap = [True]\n",
    "\n",
    "\n",
    "random_grid = {\"n_estimators\": n_estim, \n",
    "               \"max_depth\": max_depth,\n",
    "               \"min_samples_split\": min_samples_split,\n",
    "               \"min_samples_leaf\": min_samples_leaf#,\n",
    "               #\"bootstrap\": bootstrap\n",
    "              }\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = forest,\n",
    "                               param_distributions = random_grid,\n",
    "                               n_iter = 100,\n",
    "                               cv = 5, \n",
    "                               verbose = 1, \n",
    "                               n_jobs = -1,\n",
    "                               random_state = 42\n",
    "                              )\n",
    "\n",
    "rf_random.fit(x_train, y_train)\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072bd80f-711d-474b-9825-b9b8e164ff4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 225 candidates, totalling 1125 fits\n",
      "{'max_depth': 18, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 45}\n"
     ]
    }
   ],
   "source": [
    "#Using the best model from the random search, we look into closer\n",
    "#ranges of hyperparameters using a grid search\n",
    "\n",
    "\n",
    "random_grid = {\"n_estimators\": [int(x) for x in np.linspace(30, 50, num = 5)], \n",
    "               \"max_depth\": [15, 18, 20, 23, 25],\n",
    "               \"min_samples_split\": [2,3,4],\n",
    "               \"min_samples_leaf\": [2, 3, 4]\n",
    "              }\n",
    "\n",
    "rf_random = GridSearchCV(estimator = forest,\n",
    "                               param_grid = random_grid,\n",
    "                               cv = 5, \n",
    "                               verbose = 1, \n",
    "                               n_jobs = -1\n",
    "                              )\n",
    "\n",
    "rf_random.fit(x_train, y_train)\n",
    "print(rf_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf063fe7-7837-4b6d-9625-407a769fbd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 168 candidates, totalling 840 fits\n",
      "{'max_depth': 17, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 46}\n"
     ]
    }
   ],
   "source": [
    "#Using the best model from the random search, we look into closer\n",
    "#ranges of hyperparameters using a grid search\n",
    "\n",
    "\n",
    "random_grid = {\"n_estimators\": [42,43,44,45,46,47,47], \n",
    "               \"max_depth\": [16,17,18,19],\n",
    "               \"min_samples_split\": [2,3],\n",
    "               \"min_samples_leaf\": [2,3,4]\n",
    "              }\n",
    "\n",
    "rf_random = GridSearchCV(estimator = forest,\n",
    "                               param_grid = random_grid,\n",
    "                               cv = 5, \n",
    "                               verbose = 1, \n",
    "                               n_jobs = -1\n",
    "                              )\n",
    "\n",
    "rf_random.fit(x_train, y_train)\n",
    "print(rf_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de1ffd6-f4a3-4bed-9a9b-f8d92a3df719",
   "metadata": {},
   "source": [
    "### Method 2: Split training dataset per year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de914a8a-e3b4-45e0-ae2f-40989289d6a2",
   "metadata": {},
   "source": [
    "Training dataset is split in two: one part containing one year's candidates and another containing the rest. Then model trains on the latter and predicts the MVP of the first. This happens for each year between 1980 - 2021. In a very ineffective way we loop through ranges of hyperparameter values and compare with previous method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f11a00e-8ed5-4137-a7c9-3e0a3c246ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF_model = RandomForestRegressor(n_estimators = 46,\n",
    "#                                 max_depth = 17,\n",
    "#                                 min_samples_leaf = 3,\n",
    "#                                 min_samples_split = 2,\n",
    "#                                 random_state = 42\n",
    "#                                 )\n",
    "\n",
    "# model_RF, mvp_race_lst_RF, summary_RF = run_model(RF_model,\n",
    "#                                                   historical_data_reduced,\n",
    "#                                                   years,\n",
    "#                                                   target = 'Share'\n",
    "#                                                  )\n",
    "    \n",
    "# mse_RF = np.mean(summary_RF['MSE score'].values)\n",
    "# R2_RF = np.mean(summary_RF['R2 score'].values)\n",
    "# accuracy_RF = (summary_RF['Result'] == 'Right').sum() / len(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462c64c3-5ec0-4a3f-a267-2f50665464b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026513263824577034\n",
      "0.6126033396429429\n",
      "0.6904761904761905\n"
     ]
    }
   ],
   "source": [
    "# print(mse_RF)\n",
    "# print(R2_RF)\n",
    "# print(accuracy_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dd93c58-66fd-430c-a920-7a6c49730489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Actual MVP</th>\n",
       "      <th>Predicted MVP</th>\n",
       "      <th>MSE score</th>\n",
       "      <th>R2 score</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980</td>\n",
       "      <td>Kareem Abdul-Jabbar</td>\n",
       "      <td>Kareem Abdul-Jabbar</td>\n",
       "      <td>0.028840</td>\n",
       "      <td>0.286291</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1981</td>\n",
       "      <td>Julius Erving</td>\n",
       "      <td>Julius Erving</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.980476</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1982</td>\n",
       "      <td>Moses Malone</td>\n",
       "      <td>Moses Malone</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.984721</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1983</td>\n",
       "      <td>Moses Malone</td>\n",
       "      <td>Moses Malone</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.982760</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1984</td>\n",
       "      <td>Larry Bird</td>\n",
       "      <td>Larry Bird</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.983014</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1985</td>\n",
       "      <td>Larry Bird</td>\n",
       "      <td>Larry Bird</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.995226</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1986</td>\n",
       "      <td>Larry Bird</td>\n",
       "      <td>Larry Bird</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.990180</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1987</td>\n",
       "      <td>Magic Johnson</td>\n",
       "      <td>Magic Johnson</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.988917</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1988</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.965145</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1989</td>\n",
       "      <td>Magic Johnson</td>\n",
       "      <td>Magic Johnson</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.988813</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1990</td>\n",
       "      <td>Magic Johnson</td>\n",
       "      <td>Charles Barkley</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.996891</td>\n",
       "      <td>Wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1991</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.995242</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1992</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.994769</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1993</td>\n",
       "      <td>Charles Barkley</td>\n",
       "      <td>Charles Barkley</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.992431</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1994</td>\n",
       "      <td>Hakeem Olajuwon</td>\n",
       "      <td>Hakeem Olajuwon</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.995877</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1995</td>\n",
       "      <td>David Robinson</td>\n",
       "      <td>David Robinson</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.989151</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1996</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.997688</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1997</td>\n",
       "      <td>Karl Malone</td>\n",
       "      <td>Karl Malone</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.996483</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1998</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.992685</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1999</td>\n",
       "      <td>Karl Malone</td>\n",
       "      <td>Karl Malone</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.983148</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2000</td>\n",
       "      <td>Shaquille O'Neal</td>\n",
       "      <td>Shaquille O'Neal</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.989603</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2001</td>\n",
       "      <td>Allen Iverson</td>\n",
       "      <td>Allen Iverson</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.993425</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2002</td>\n",
       "      <td>Tim Duncan</td>\n",
       "      <td>Tim Duncan</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.995204</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2003</td>\n",
       "      <td>Tim Duncan</td>\n",
       "      <td>Tim Duncan</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.998341</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2004</td>\n",
       "      <td>Kevin Garnett</td>\n",
       "      <td>Kevin Garnett</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.994561</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2005</td>\n",
       "      <td>Steve Nash</td>\n",
       "      <td>Steve Nash</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.994823</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2006</td>\n",
       "      <td>Steve Nash</td>\n",
       "      <td>Steve Nash</td>\n",
       "      <td>0.011373</td>\n",
       "      <td>0.820859</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2007</td>\n",
       "      <td>Dirk Nowitzki</td>\n",
       "      <td>Dirk Nowitzki</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2008</td>\n",
       "      <td>Kobe Bryant</td>\n",
       "      <td>Kobe Bryant</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.998149</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2009</td>\n",
       "      <td>LeBron James</td>\n",
       "      <td>LeBron James</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.997262</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2010</td>\n",
       "      <td>LeBron James</td>\n",
       "      <td>LeBron James</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.995182</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2011</td>\n",
       "      <td>Derrick Rose</td>\n",
       "      <td>Derrick Rose</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.997475</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2012</td>\n",
       "      <td>LeBron James</td>\n",
       "      <td>LeBron James</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.996185</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2013</td>\n",
       "      <td>LeBron James</td>\n",
       "      <td>LeBron James</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.997699</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2014</td>\n",
       "      <td>Kevin Durant</td>\n",
       "      <td>Kevin Durant</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.985215</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2015</td>\n",
       "      <td>Stephen Curry</td>\n",
       "      <td>Stephen Curry</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.991059</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2016</td>\n",
       "      <td>Stephen Curry</td>\n",
       "      <td>Stephen Curry</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.981951</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2017</td>\n",
       "      <td>Russell Westbrook</td>\n",
       "      <td>Russell Westbrook</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.992228</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018</td>\n",
       "      <td>James Harden</td>\n",
       "      <td>James Harden</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.988650</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2019</td>\n",
       "      <td>Giannis Antetokounmpo</td>\n",
       "      <td>Giannis Antetokounmpo</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.995544</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2020</td>\n",
       "      <td>Giannis Antetokounmpo</td>\n",
       "      <td>Giannis Antetokounmpo</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.989437</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2021</td>\n",
       "      <td>Nikola Jokić</td>\n",
       "      <td>Nikola Jokić</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.995584</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year             Actual MVP          Predicted MVP  MSE score  R2 score  \\\n",
       "0   1980    Kareem Abdul-Jabbar    Kareem Abdul-Jabbar   0.028840  0.286291   \n",
       "1   1981          Julius Erving          Julius Erving   0.000550  0.980476   \n",
       "2   1982           Moses Malone           Moses Malone   0.000509  0.984721   \n",
       "3   1983           Moses Malone           Moses Malone   0.000862  0.982760   \n",
       "4   1984             Larry Bird             Larry Bird   0.000902  0.983014   \n",
       "5   1985             Larry Bird             Larry Bird   0.000210  0.995226   \n",
       "6   1986             Larry Bird             Larry Bird   0.000578  0.990180   \n",
       "7   1987          Magic Johnson          Magic Johnson   0.000713  0.988917   \n",
       "8   1988         Michael Jordan         Michael Jordan   0.002401  0.965145   \n",
       "9   1989          Magic Johnson          Magic Johnson   0.000617  0.988813   \n",
       "10  1990          Magic Johnson        Charles Barkley   0.000204  0.996891   \n",
       "11  1991         Michael Jordan         Michael Jordan   0.000267  0.995242   \n",
       "12  1992         Michael Jordan         Michael Jordan   0.000324  0.994769   \n",
       "13  1993        Charles Barkley        Charles Barkley   0.000622  0.992431   \n",
       "14  1994        Hakeem Olajuwon        Hakeem Olajuwon   0.000302  0.995877   \n",
       "15  1995         David Robinson         David Robinson   0.000720  0.989151   \n",
       "16  1996         Michael Jordan         Michael Jordan   0.000142  0.997688   \n",
       "17  1997            Karl Malone            Karl Malone   0.000223  0.996483   \n",
       "18  1998         Michael Jordan         Michael Jordan   0.000491  0.992685   \n",
       "19  1999            Karl Malone            Karl Malone   0.000876  0.983148   \n",
       "20  2000       Shaquille O'Neal       Shaquille O'Neal   0.000610  0.989603   \n",
       "21  2001          Allen Iverson          Allen Iverson   0.000447  0.993425   \n",
       "22  2002             Tim Duncan             Tim Duncan   0.000297  0.995204   \n",
       "23  2003             Tim Duncan             Tim Duncan   0.000126  0.998341   \n",
       "24  2004          Kevin Garnett          Kevin Garnett   0.000402  0.994561   \n",
       "25  2005             Steve Nash             Steve Nash   0.000381  0.994823   \n",
       "26  2006             Steve Nash             Steve Nash   0.011373  0.820859   \n",
       "27  2007          Dirk Nowitzki          Dirk Nowitzki   0.000603  0.991716   \n",
       "28  2008            Kobe Bryant            Kobe Bryant   0.000139  0.998149   \n",
       "29  2009           LeBron James           LeBron James   0.000257  0.997262   \n",
       "30  2010           LeBron James           LeBron James   0.000388  0.995182   \n",
       "31  2011           Derrick Rose           Derrick Rose   0.000206  0.997475   \n",
       "32  2012           LeBron James           LeBron James   0.000290  0.996185   \n",
       "33  2013           LeBron James           LeBron James   0.000176  0.997699   \n",
       "34  2014           Kevin Durant           Kevin Durant   0.001114  0.985215   \n",
       "35  2015          Stephen Curry          Stephen Curry   0.000814  0.991059   \n",
       "36  2016          Stephen Curry          Stephen Curry   0.001713  0.981951   \n",
       "37  2017      Russell Westbrook      Russell Westbrook   0.000761  0.992228   \n",
       "38  2018           James Harden           James Harden   0.001030  0.988650   \n",
       "39  2019  Giannis Antetokounmpo  Giannis Antetokounmpo   0.000409  0.995544   \n",
       "40  2020  Giannis Antetokounmpo  Giannis Antetokounmpo   0.000974  0.989437   \n",
       "41  2021           Nikola Jokić           Nikola Jokić   0.000369  0.995584   \n",
       "\n",
       "   Result  \n",
       "0   Right  \n",
       "1   Right  \n",
       "2   Right  \n",
       "3   Right  \n",
       "4   Right  \n",
       "5   Right  \n",
       "6   Right  \n",
       "7   Right  \n",
       "8   Right  \n",
       "9   Right  \n",
       "10  Wrong  \n",
       "11  Right  \n",
       "12  Right  \n",
       "13  Right  \n",
       "14  Right  \n",
       "15  Right  \n",
       "16  Right  \n",
       "17  Right  \n",
       "18  Right  \n",
       "19  Right  \n",
       "20  Right  \n",
       "21  Right  \n",
       "22  Right  \n",
       "23  Right  \n",
       "24  Right  \n",
       "25  Right  \n",
       "26  Right  \n",
       "27  Right  \n",
       "28  Right  \n",
       "29  Right  \n",
       "30  Right  \n",
       "31  Right  \n",
       "32  Right  \n",
       "33  Right  \n",
       "34  Right  \n",
       "35  Right  \n",
       "36  Right  \n",
       "37  Right  \n",
       "38  Right  \n",
       "39  Right  \n",
       "40  Right  \n",
       "41  Right  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summaries_RF[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd0babc8-79f8-4dba-b59c-92ec53ab1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_RF = {'n_estimators': trees_RF_lst,\n",
    "#          'min_leafs': leaf_RF_lst,\n",
    "#          'min_split': split_RF_lst,\n",
    "#          'max_depth': max_depth_RF_lst,\n",
    "#          'MSE': mse_avg_RF_lst,\n",
    "#          'R2': R2_avg_RF_lst,\n",
    "#          'Accuracy': acc_RF_lst\n",
    "#          }\n",
    "\n",
    "# df_RF = pd.DataFrame(ds_RF).sort_values([\"Accuracy\", \"R2\"], ascending = (False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e68eb2ac-3289-4a00-942e-b4fda90a35d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_leafs</th>\n",
       "      <th>min_split</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026616</td>\n",
       "      <td>0.610668</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.026621</td>\n",
       "      <td>0.610419</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026639</td>\n",
       "      <td>0.610374</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.026644</td>\n",
       "      <td>0.610138</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026751</td>\n",
       "      <td>0.608954</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.026768</td>\n",
       "      <td>0.608517</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026818</td>\n",
       "      <td>0.607953</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026811</td>\n",
       "      <td>0.607938</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0.026826</td>\n",
       "      <td>0.607861</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0.026818</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.027015</td>\n",
       "      <td>0.605563</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0.027071</td>\n",
       "      <td>0.605292</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.027072</td>\n",
       "      <td>0.604931</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0.027123</td>\n",
       "      <td>0.604404</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.027111</td>\n",
       "      <td>0.603959</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.027137</td>\n",
       "      <td>0.603881</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0.027212</td>\n",
       "      <td>0.603240</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.027203</td>\n",
       "      <td>0.602717</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.027227</td>\n",
       "      <td>0.602680</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026732</td>\n",
       "      <td>0.609004</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.026746</td>\n",
       "      <td>0.608783</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026778</td>\n",
       "      <td>0.608368</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>0.608160</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026811</td>\n",
       "      <td>0.608015</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.026827</td>\n",
       "      <td>0.607781</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026908</td>\n",
       "      <td>0.606769</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0.026916</td>\n",
       "      <td>0.606682</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0.026893</td>\n",
       "      <td>0.606224</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026905</td>\n",
       "      <td>0.606034</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0.026930</td>\n",
       "      <td>0.605645</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026942</td>\n",
       "      <td>0.605460</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0.026956</td>\n",
       "      <td>0.605402</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.026969</td>\n",
       "      <td>0.605204</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.027099</td>\n",
       "      <td>0.603994</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.027193</td>\n",
       "      <td>0.602364</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.027286</td>\n",
       "      <td>0.601127</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  min_leafs  min_split  max_depth       MSE        R2  \\\n",
       "26            52          3         10         17  0.026616  0.610668   \n",
       "32            52          3         10         18  0.026621  0.610419   \n",
       "14            50          3         10         17  0.026639  0.610374   \n",
       "20            50          3         10         18  0.026644  0.610138   \n",
       "2             48          3         10         17  0.026751  0.608954   \n",
       "8             48          3         10         18  0.026768  0.608517   \n",
       "15            50          3         11         17  0.026818  0.607953   \n",
       "27            52          3         11         17  0.026811  0.607938   \n",
       "21            50          3         11         18  0.026826  0.607861   \n",
       "33            52          3         11         18  0.026818  0.607850   \n",
       "24            52          2         10         17  0.027015  0.605563   \n",
       "31            52          2         11         18  0.027071  0.605292   \n",
       "25            52          2         11         17  0.027072  0.604931   \n",
       "19            50          2         11         18  0.027123  0.604404   \n",
       "12            50          2         10         17  0.027111  0.603959   \n",
       "13            50          2         11         17  0.027137  0.603881   \n",
       "7             48          2         11         18  0.027212  0.603240   \n",
       "0             48          2         10         17  0.027203  0.602717   \n",
       "1             48          2         11         17  0.027227  0.602680   \n",
       "16            50          4         10         17  0.026732  0.609004   \n",
       "22            50          4         10         18  0.026746  0.608783   \n",
       "28            52          4         10         17  0.026778  0.608368   \n",
       "34            52          4         10         18  0.026791  0.608160   \n",
       "4             48          4         10         17  0.026811  0.608015   \n",
       "10            48          4         10         18  0.026827  0.607781   \n",
       "3             48          3         11         17  0.026908  0.606769   \n",
       "9             48          3         11         18  0.026916  0.606682   \n",
       "23            50          4         11         18  0.026893  0.606224   \n",
       "17            50          4         11         17  0.026905  0.606034   \n",
       "35            52          4         11         18  0.026930  0.605645   \n",
       "29            52          4         11         17  0.026942  0.605460   \n",
       "11            48          4         11         18  0.026956  0.605402   \n",
       "5             48          4         11         17  0.026969  0.605204   \n",
       "30            52          2         10         18  0.027099  0.603994   \n",
       "18            50          2         10         18  0.027193  0.602364   \n",
       "6             48          2         10         18  0.027286  0.601127   \n",
       "\n",
       "    Accuracy  \n",
       "26  0.690476  \n",
       "32  0.690476  \n",
       "14  0.690476  \n",
       "20  0.690476  \n",
       "2   0.690476  \n",
       "8   0.690476  \n",
       "15  0.690476  \n",
       "27  0.690476  \n",
       "21  0.690476  \n",
       "33  0.690476  \n",
       "24  0.690476  \n",
       "31  0.690476  \n",
       "25  0.690476  \n",
       "19  0.690476  \n",
       "12  0.690476  \n",
       "13  0.690476  \n",
       "7   0.690476  \n",
       "0   0.690476  \n",
       "1   0.690476  \n",
       "16  0.666667  \n",
       "22  0.666667  \n",
       "28  0.666667  \n",
       "34  0.666667  \n",
       "4   0.666667  \n",
       "10  0.666667  \n",
       "3   0.666667  \n",
       "9   0.666667  \n",
       "23  0.666667  \n",
       "17  0.666667  \n",
       "35  0.666667  \n",
       "29  0.666667  \n",
       "11  0.666667  \n",
       "5   0.666667  \n",
       "30  0.666667  \n",
       "18  0.666667  \n",
       "6   0.666667  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7efa74-9f43-4800-aae7-70c2fd98ced0",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for XGB Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278cadf-8562-4cf8-a2fe-b9b25e0ffff6",
   "metadata": {},
   "source": [
    "### Method 1: As previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "519806e9-3201-49ee-9b21-607adf3da000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "{'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 32}\n"
     ]
    }
   ],
   "source": [
    " #number of trees used in sequence\n",
    "trees_grid = [20,23,27,32,36,40,45,50]\n",
    "# value of learning rate used by gradient descent\n",
    "alpha_grid = [0.15,0.18,0.21,0.25]\n",
    "#maximum depth allowed for each tree\n",
    "max_depth_grid = [2,3,4]\n",
    "\n",
    "xgb_grid = {'n_estimators': trees_grid,\n",
    "            'learning_rate': alpha_grid,\n",
    "            'max_depth': max_depth_grid\n",
    "            }\n",
    "\n",
    "xgb_regressor = XGBRegressor()\n",
    "\n",
    "xgb_search = GridSearchCV(estimator = xgb_regressor,\n",
    "                          param_grid = xgb_grid,\n",
    "                          cv = 5, \n",
    "                          verbose = 1, \n",
    "                          n_jobs = -1\n",
    "                         )\n",
    "\n",
    "xgb_search.fit(x_train, y_train)\n",
    "print(xgb_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07ee5ed0-af33-4c4b-908c-633cb3745977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 168 candidates, totalling 840 fits\n",
      "{'learning_rate': 0.16, 'max_depth': 3, 'n_estimators': 42}\n"
     ]
    }
   ],
   "source": [
    "#number of trees used in sequence\n",
    "trees_grid = [30,32,35,37,40,42,45]\n",
    "# value of learning rate used by gradient descent\n",
    "alpha_grid = [0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.22, 0.25]\n",
    "#maximum depth allowed for each tree\n",
    "max_depth_grid = [2,3,4]\n",
    "\n",
    "xgb_grid = {'n_estimators': trees_grid,\n",
    "            'learning_rate': alpha_grid,\n",
    "            'max_depth': max_depth_grid\n",
    "            }\n",
    "\n",
    "xgb_regressor = XGBRegressor()\n",
    "\n",
    "xgb_search = GridSearchCV(estimator = xgb_regressor,\n",
    "                          param_grid = xgb_grid,\n",
    "                          cv = 5, \n",
    "                          verbose = 1, \n",
    "                          n_jobs = -1\n",
    "                         )\n",
    "\n",
    "xgb_search.fit(x_train, y_train)\n",
    "print(xgb_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c287145-3356-4721-87cb-3d95ddb347ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "{'learning_rate': 0.16, 'max_depth': 3, 'n_estimators': 43}\n"
     ]
    }
   ],
   "source": [
    " #number of trees used in sequence\n",
    "trees_grid = [40,41,42,43,44,45]\n",
    "# value of learning rate used by gradient descent\n",
    "alpha_grid = [0.15, 0.16,0.17, 0.18, 0.19, 0.2, 0.22, 0.25]\n",
    "#maximum depth allowed for each tree\n",
    "max_depth_grid = [2,3]\n",
    "\n",
    "xgb_grid = {'n_estimators': trees_grid,\n",
    "            'learning_rate': alpha_grid,\n",
    "            'max_depth': max_depth_grid\n",
    "            }\n",
    "\n",
    "xgb_regressor = XGBRegressor()\n",
    "\n",
    "xgb_search = GridSearchCV(estimator = xgb_regressor,\n",
    "                          param_grid = xgb_grid,\n",
    "                          cv = 5, \n",
    "                          verbose = 1, \n",
    "                          n_jobs = -1\n",
    "                         )\n",
    "\n",
    "xgb_search.fit(x_train, y_train)\n",
    "print(xgb_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b07ad6-9ddc-41da-80f6-040f645242fd",
   "metadata": {},
   "source": [
    "### Method 2: As previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec52346c-75ab-4459-b6bb-1b26325ea7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Using a different cross-validation technique for hyperparameter tuning,\n",
    "# #using the run_model function, evaluating for each set of hyperparameters\n",
    "# #and sorting results by accuracy and R2 score\n",
    "\n",
    "\n",
    "# trees_XGB_grid = [13,17,20,23,27,32,36,40,50]\n",
    "# alpha_XGB_grid = [0.1,0.15,0.18,0.21,0.25]\n",
    "# max_depth_XGB_grid = [2,4,6,7]\n",
    "\n",
    "\n",
    "# mse_avg_XGB_lst = []\n",
    "# R2_avg_XGB_lst = []\n",
    "# acc_XGB_lst = []\n",
    "# trees_XGB_lst = []\n",
    "# max_depth_XGB_lst = []\n",
    "# alpha_XGB_lst = []\n",
    "\n",
    "# summaries_XGB = []\n",
    "\n",
    "# for trees in trees_XGB_grid:\n",
    "#     for alpha in alpha_XGB_grid:\n",
    "#         for max_depth in max_depth_XGB_grid:\n",
    "#             XGB_grid = XGBRegressor(n_estimators = trees,\n",
    "#                                     learning_rate = alpha,\n",
    "#                                     max_depth = max_depth\n",
    "#                                    )\n",
    "            \n",
    "#             model_XGB, mvp_race_lst_XGB, summary_XGB = run_model(XGB_grid,\n",
    "#                                                                  historical_data_reduced,\n",
    "#                                                                  years,\n",
    "#                                                                  target='Share'\n",
    "#                                                                 )\n",
    "    \n",
    "#             mse_avg_XGB = np.mean(summary_XGB['MSE score'].values)\n",
    "#             R2_avg_XGB = np.mean(summary_XGB['R2 score'].values)\n",
    "#             accuracy_XGB = (summary_XGB['Result'] == 'Right').sum() / len(years)\n",
    "    \n",
    "#             trees_XGB_lst.append(trees)\n",
    "#             max_depth_XGB_lst.append(max_depth)\n",
    "#             alpha_XGB_lst.append(alpha)\n",
    "    \n",
    "#             mse_avg_XGB_lst.append(mse_avg_XGB)\n",
    "#             R2_avg_XGB_lst.append(R2_avg_XGB)\n",
    "#             acc_XGB_lst.append(accuracy_XGB)\n",
    "    \n",
    "# ds_XGB = {'n_estimators': trees_XGB_lst,\n",
    "#           'learning_rate': alpha_XGB_lst,\n",
    "#           'max_depth': max_depth_XGB_lst,\n",
    "#           'MSE': mse_avg_XGB_lst,\n",
    "#           'R2': R2_avg_XGB_lst,\n",
    "#           'Accuracy': acc_XGB_lst\n",
    "#          }\n",
    "\n",
    "# df_XGB = pd.DataFrame(ds_XGB).sort_values([\"Accuracy\", \"R2\"], ascending = (False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16ed7f6a-bba4-4743-883e-284ea395d678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>50</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.026799</td>\n",
       "      <td>0.606848</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.026958</td>\n",
       "      <td>0.603458</td>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>50</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027781</td>\n",
       "      <td>0.597778</td>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4</td>\n",
       "      <td>0.027755</td>\n",
       "      <td>0.594421</td>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027767</td>\n",
       "      <td>0.593559</td>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>20</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028009</td>\n",
       "      <td>0.591407</td>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028260</td>\n",
       "      <td>0.586848</td>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>36</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028458</td>\n",
       "      <td>0.584273</td>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028730</td>\n",
       "      <td>0.577678</td>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.561420</td>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031954</td>\n",
       "      <td>0.533072</td>\n",
       "      <td>0.738095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027044</td>\n",
       "      <td>0.603903</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>36</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.601668</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027334</td>\n",
       "      <td>0.599860</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4</td>\n",
       "      <td>0.027419</td>\n",
       "      <td>0.599799</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>32</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027331</td>\n",
       "      <td>0.598557</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>32</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4</td>\n",
       "      <td>0.027582</td>\n",
       "      <td>0.597731</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027470</td>\n",
       "      <td>0.597007</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>40</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027805</td>\n",
       "      <td>0.596871</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>36</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027531</td>\n",
       "      <td>0.596023</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027723</td>\n",
       "      <td>0.593598</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.027912</td>\n",
       "      <td>0.592257</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028008</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027963</td>\n",
       "      <td>0.589843</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>32</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028052</td>\n",
       "      <td>0.589159</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>50</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028290</td>\n",
       "      <td>0.587722</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028230</td>\n",
       "      <td>0.586901</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>27</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028292</td>\n",
       "      <td>0.585299</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>23</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028565</td>\n",
       "      <td>0.584926</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028431</td>\n",
       "      <td>0.584568</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>20</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028503</td>\n",
       "      <td>0.582575</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028756</td>\n",
       "      <td>0.580370</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028746</td>\n",
       "      <td>0.578805</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028660</td>\n",
       "      <td>0.578124</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029117</td>\n",
       "      <td>0.574972</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>32</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.029746</td>\n",
       "      <td>0.567551</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.029744</td>\n",
       "      <td>0.567482</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>36</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.029838</td>\n",
       "      <td>0.566095</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.029915</td>\n",
       "      <td>0.565158</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>17</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.029904</td>\n",
       "      <td>0.565090</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>40</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.029930</td>\n",
       "      <td>0.564472</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>23</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.029980</td>\n",
       "      <td>0.564260</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>50</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030090</td>\n",
       "      <td>0.562445</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031133</td>\n",
       "      <td>0.541563</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>32</td>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031180</td>\n",
       "      <td>0.541321</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031495</td>\n",
       "      <td>0.540873</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031611</td>\n",
       "      <td>0.535042</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>23</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031628</td>\n",
       "      <td>0.534995</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>36</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031706</td>\n",
       "      <td>0.534481</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>32</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031715</td>\n",
       "      <td>0.534368</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>40</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031724</td>\n",
       "      <td>0.534212</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031709</td>\n",
       "      <td>0.534056</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>50</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031745</td>\n",
       "      <td>0.534026</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031765</td>\n",
       "      <td>0.533532</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031789</td>\n",
       "      <td>0.533000</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>17</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031787</td>\n",
       "      <td>0.532588</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>23</td>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.032273</td>\n",
       "      <td>0.530987</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "      <td>0.032495</td>\n",
       "      <td>0.523123</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.037495</td>\n",
       "      <td>0.450751</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.043981</td>\n",
       "      <td>0.350221</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027396</td>\n",
       "      <td>0.598427</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>36</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027784</td>\n",
       "      <td>0.596945</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>36</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.596583</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027605</td>\n",
       "      <td>0.595260</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>36</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4</td>\n",
       "      <td>0.027788</td>\n",
       "      <td>0.594770</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>32</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027952</td>\n",
       "      <td>0.594231</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027865</td>\n",
       "      <td>0.591897</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027884</td>\n",
       "      <td>0.591576</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028014</td>\n",
       "      <td>0.589538</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>36</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028246</td>\n",
       "      <td>0.585540</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>32</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028237</td>\n",
       "      <td>0.585107</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028280</td>\n",
       "      <td>0.584343</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028465</td>\n",
       "      <td>0.584168</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029015</td>\n",
       "      <td>0.579029</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>36</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028605</td>\n",
       "      <td>0.578987</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>36</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028956</td>\n",
       "      <td>0.577745</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>32</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028708</td>\n",
       "      <td>0.577407</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028741</td>\n",
       "      <td>0.576478</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028945</td>\n",
       "      <td>0.576150</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>36</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028742</td>\n",
       "      <td>0.575854</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>27</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028832</td>\n",
       "      <td>0.575114</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028968</td>\n",
       "      <td>0.572942</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>27</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.029405</td>\n",
       "      <td>0.571286</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029543</td>\n",
       "      <td>0.569575</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029565</td>\n",
       "      <td>0.567274</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030256</td>\n",
       "      <td>0.559977</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>36</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030245</td>\n",
       "      <td>0.558633</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030253</td>\n",
       "      <td>0.558293</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>32</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030286</td>\n",
       "      <td>0.558230</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>50</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030280</td>\n",
       "      <td>0.558204</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030404</td>\n",
       "      <td>0.556688</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030396</td>\n",
       "      <td>0.556580</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030568</td>\n",
       "      <td>0.555650</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>36</td>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030629</td>\n",
       "      <td>0.554843</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>23</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030711</td>\n",
       "      <td>0.552396</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>20</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030702</td>\n",
       "      <td>0.551937</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030795</td>\n",
       "      <td>0.551720</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030934</td>\n",
       "      <td>0.550427</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.030754</td>\n",
       "      <td>0.549989</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>27</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030982</td>\n",
       "      <td>0.549151</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030958</td>\n",
       "      <td>0.547335</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030921</td>\n",
       "      <td>0.547266</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031051</td>\n",
       "      <td>0.546874</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.030997</td>\n",
       "      <td>0.546760</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>36</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031060</td>\n",
       "      <td>0.545827</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031054</td>\n",
       "      <td>0.545804</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>36</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031101</td>\n",
       "      <td>0.545316</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>32</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031159</td>\n",
       "      <td>0.545217</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031154</td>\n",
       "      <td>0.544219</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>27</td>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031509</td>\n",
       "      <td>0.542314</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031119</td>\n",
       "      <td>0.541523</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>36</td>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031206</td>\n",
       "      <td>0.541019</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031437</td>\n",
       "      <td>0.540487</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>27</td>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031310</td>\n",
       "      <td>0.539390</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031989</td>\n",
       "      <td>0.533718</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>27</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031923</td>\n",
       "      <td>0.533405</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032213</td>\n",
       "      <td>0.530283</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.032332</td>\n",
       "      <td>0.525252</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>23</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032867</td>\n",
       "      <td>0.521621</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.032939</td>\n",
       "      <td>0.516928</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033096</td>\n",
       "      <td>0.515998</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033050</td>\n",
       "      <td>0.515901</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>36</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033157</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.515285</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.515252</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033041</td>\n",
       "      <td>0.515230</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>23</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.515199</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.514916</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033120</td>\n",
       "      <td>0.514158</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.033662</td>\n",
       "      <td>0.509984</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.034452</td>\n",
       "      <td>0.495856</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.034616</td>\n",
       "      <td>0.491083</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034897</td>\n",
       "      <td>0.491045</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034848</td>\n",
       "      <td>0.489945</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.035702</td>\n",
       "      <td>0.476634</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.042012</td>\n",
       "      <td>0.377760</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.042650</td>\n",
       "      <td>0.367799</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028071</td>\n",
       "      <td>0.592625</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028161</td>\n",
       "      <td>0.585146</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>23</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028238</td>\n",
       "      <td>0.584393</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028247</td>\n",
       "      <td>0.582207</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028355</td>\n",
       "      <td>0.582110</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028282</td>\n",
       "      <td>0.582094</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028463</td>\n",
       "      <td>0.580569</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028838</td>\n",
       "      <td>0.575009</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028939</td>\n",
       "      <td>0.574205</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>17</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029546</td>\n",
       "      <td>0.572444</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>32</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030050</td>\n",
       "      <td>0.566120</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>27</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030205</td>\n",
       "      <td>0.563832</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.029745</td>\n",
       "      <td>0.562719</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030587</td>\n",
       "      <td>0.550555</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031155</td>\n",
       "      <td>0.548113</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030891</td>\n",
       "      <td>0.547875</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030927</td>\n",
       "      <td>0.547544</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031042</td>\n",
       "      <td>0.545978</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031495</td>\n",
       "      <td>0.545458</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.544587</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>36</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031215</td>\n",
       "      <td>0.544424</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031256</td>\n",
       "      <td>0.543783</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031269</td>\n",
       "      <td>0.543515</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>50</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031370</td>\n",
       "      <td>0.542037</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>20</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0.031461</td>\n",
       "      <td>0.541159</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.032879</td>\n",
       "      <td>0.518124</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033454</td>\n",
       "      <td>0.511997</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034581</td>\n",
       "      <td>0.493806</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.036860</td>\n",
       "      <td>0.459203</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.041466</td>\n",
       "      <td>0.386945</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>17</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028116</td>\n",
       "      <td>0.586276</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>50</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028328</td>\n",
       "      <td>0.585864</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>40</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028339</td>\n",
       "      <td>0.584697</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028351</td>\n",
       "      <td>0.582235</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.029017</td>\n",
       "      <td>0.573717</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.029882</td>\n",
       "      <td>0.568823</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>36</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030006</td>\n",
       "      <td>0.567135</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030011</td>\n",
       "      <td>0.566322</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030331</td>\n",
       "      <td>0.561434</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030653</td>\n",
       "      <td>0.557115</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030908</td>\n",
       "      <td>0.548109</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>0.546232</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.034013</td>\n",
       "      <td>0.508590</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_estimators  learning_rate  max_depth       MSE        R2  Accuracy\n",
       "168            50           0.18          2  0.026799  0.606848  0.761905\n",
       "148            40           0.18          2  0.026958  0.603458  0.738095\n",
       "172            50           0.21          2  0.027781  0.597778  0.738095\n",
       "69             23           0.18          4  0.027755  0.594421  0.738095\n",
       "160            50           0.10          2  0.027767  0.593559  0.738095\n",
       "49             20           0.18          4  0.028009  0.591407  0.738095\n",
       "141            40           0.10          4  0.028260  0.586848  0.738095\n",
       "121            36           0.10          4  0.028458  0.584273  0.738095\n",
       "165            50           0.15          4  0.028730  0.577678  0.738095\n",
       "9              13           0.18          4  0.030000  0.561420  0.738095\n",
       "10             13           0.18          6  0.031954  0.533072  0.738095\n",
       "164            50           0.15          2  0.027044  0.603903  0.714286\n",
       "128            36           0.18          2  0.027100  0.601668  0.714286\n",
       "176            50           0.25          2  0.027334  0.599860  0.714286\n",
       "89             27           0.18          4  0.027419  0.599799  0.714286\n",
       "108            32           0.18          2  0.027331  0.598557  0.714286\n",
       "109            32           0.18          4  0.027582  0.597731  0.714286\n",
       "144            40           0.15          2  0.027470  0.597007  0.714286\n",
       "152            40           0.21          2  0.027805  0.596871  0.714286\n",
       "124            36           0.15          2  0.027531  0.596023  0.714286\n",
       "88             27           0.18          2  0.027723  0.593598  0.714286\n",
       "161            50           0.10          4  0.027912  0.592257  0.714286\n",
       "149            40           0.18          4  0.028008  0.591667  0.714286\n",
       "68             23           0.18          2  0.027963  0.589843  0.714286\n",
       "104            32           0.15          2  0.028052  0.589159  0.714286\n",
       "169            50           0.18          4  0.028290  0.587722  0.714286\n",
       "36             17           0.25          2  0.028230  0.586901  0.714286\n",
       "84             27           0.15          2  0.028292  0.585299  0.714286\n",
       "72             23           0.21          2  0.028565  0.584926  0.714286\n",
       "29             17           0.18          4  0.028431  0.584568  0.714286\n",
       "48             20           0.18          2  0.028503  0.582575  0.714286\n",
       "101            32           0.10          4  0.028756  0.580370  0.714286\n",
       "64             23           0.15          2  0.028746  0.578805  0.714286\n",
       "145            40           0.15          4  0.028660  0.578124  0.714286\n",
       "28             17           0.18          2  0.029117  0.574972  0.714286\n",
       "114            32           0.21          6  0.029746  0.567551  0.714286\n",
       "94             27           0.21          6  0.029744  0.567482  0.714286\n",
       "134            36           0.21          6  0.029838  0.566095  0.714286\n",
       "54             20           0.21          6  0.029915  0.565158  0.714286\n",
       "34             17           0.21          6  0.029904  0.565090  0.714286\n",
       "154            40           0.21          6  0.029930  0.564472  0.714286\n",
       "74             23           0.21          6  0.029980  0.564260  0.714286\n",
       "174            50           0.21          6  0.030090  0.562445  0.714286\n",
       "147            40           0.15          7  0.031133  0.541563  0.714286\n",
       "107            32           0.15          7  0.031180  0.541321  0.714286\n",
       "8              13           0.18          2  0.031495  0.540873  0.714286\n",
       "67             23           0.15          7  0.031611  0.535042  0.714286\n",
       "75             23           0.21          7  0.031628  0.534995  0.714286\n",
       "135            36           0.21          7  0.031706  0.534481  0.714286\n",
       "115            32           0.21          7  0.031715  0.534368  0.714286\n",
       "155            40           0.21          7  0.031724  0.534212  0.714286\n",
       "55             20           0.21          7  0.031709  0.534056  0.714286\n",
       "175            50           0.21          7  0.031745  0.534026  0.714286\n",
       "95             27           0.21          7  0.031765  0.533532  0.714286\n",
       "47             20           0.15          7  0.031789  0.533000  0.714286\n",
       "35             17           0.21          7  0.031787  0.532588  0.714286\n",
       "62             23           0.10          6  0.032273  0.530987  0.714286\n",
       "15             13           0.21          7  0.032495  0.523123  0.714286\n",
       "20             17           0.10          2  0.037495  0.450751  0.714286\n",
       "0              13           0.10          2  0.043981  0.350221  0.714286\n",
       "156            40           0.25          2  0.027396  0.598427  0.690476\n",
       "132            36           0.21          2  0.027784  0.596945  0.690476\n",
       "136            36           0.25          2  0.027514  0.596583  0.690476\n",
       "116            32           0.25          2  0.027605  0.595260  0.690476\n",
       "129            36           0.18          4  0.027788  0.594770  0.690476\n",
       "112            32           0.21          2  0.027952  0.594231  0.690476\n",
       "76             23           0.25          2  0.027865  0.591897  0.690476\n",
       "96             27           0.25          2  0.027884  0.591576  0.690476\n",
       "56             20           0.25          2  0.028014  0.589538  0.690476\n",
       "133            36           0.21          4  0.028246  0.585540  0.690476\n",
       "113            32           0.21          4  0.028237  0.585107  0.690476\n",
       "93             27           0.21          4  0.028280  0.584343  0.690476\n",
       "140            40           0.10          2  0.028465  0.584168  0.690476\n",
       "52             20           0.21          2  0.029015  0.579029  0.690476\n",
       "125            36           0.15          4  0.028605  0.578987  0.690476\n",
       "120            36           0.10          2  0.028956  0.577745  0.690476\n",
       "105            32           0.15          4  0.028708  0.577407  0.690476\n",
       "117            32           0.25          4  0.028741  0.576478  0.690476\n",
       "16             13           0.25          2  0.028945  0.576150  0.690476\n",
       "137            36           0.25          4  0.028742  0.575854  0.690476\n",
       "85             27           0.15          4  0.028832  0.575114  0.690476\n",
       "65             23           0.15          4  0.028968  0.572942  0.690476\n",
       "81             27           0.10          4  0.029405  0.571286  0.690476\n",
       "100            32           0.10          2  0.029543  0.569575  0.690476\n",
       "44             20           0.15          2  0.029565  0.567274  0.690476\n",
       "162            50           0.10          6  0.030256  0.559977  0.690476\n",
       "130            36           0.18          6  0.030245  0.558633  0.690476\n",
       "150            40           0.18          6  0.030253  0.558293  0.690476\n",
       "110            32           0.18          6  0.030286  0.558230  0.690476\n",
       "170            50           0.18          6  0.030280  0.558204  0.690476\n",
       "90             27           0.18          6  0.030404  0.556688  0.690476\n",
       "70             23           0.18          6  0.030396  0.556580  0.690476\n",
       "142            40           0.10          6  0.030568  0.555650  0.690476\n",
       "122            36           0.10          6  0.030629  0.554843  0.690476\n",
       "61             23           0.10          4  0.030711  0.552396  0.690476\n",
       "50             20           0.18          6  0.030702  0.551937  0.690476\n",
       "14             13           0.21          6  0.030795  0.551720  0.690476\n",
       "102            32           0.10          6  0.030934  0.550427  0.690476\n",
       "163            50           0.10          7  0.030754  0.549989  0.690476\n",
       "80             27           0.10          2  0.030982  0.549151  0.690476\n",
       "118            32           0.25          6  0.030958  0.547335  0.690476\n",
       "24             17           0.15          2  0.030921  0.547266  0.690476\n",
       "30             17           0.18          6  0.031051  0.546874  0.690476\n",
       "143            40           0.10          7  0.030997  0.546760  0.690476\n",
       "138            36           0.25          6  0.031060  0.545827  0.690476\n",
       "158            40           0.25          6  0.031054  0.545804  0.690476\n",
       "123            36           0.10          7  0.031101  0.545316  0.690476\n",
       "111            32           0.18          7  0.031159  0.545217  0.690476\n",
       "178            50           0.25          6  0.031154  0.544219  0.690476\n",
       "82             27           0.10          6  0.031509  0.542314  0.690476\n",
       "167            50           0.15          7  0.031119  0.541523  0.690476\n",
       "127            36           0.15          7  0.031206  0.541019  0.690476\n",
       "103            32           0.10          7  0.031437  0.540487  0.690476\n",
       "87             27           0.15          7  0.031310  0.539390  0.690476\n",
       "31             17           0.18          7  0.031989  0.533718  0.690476\n",
       "83             27           0.10          7  0.031923  0.533405  0.690476\n",
       "41             20           0.10          4  0.032213  0.530283  0.690476\n",
       "27             17           0.15          7  0.032332  0.525252  0.690476\n",
       "60             23           0.10          2  0.032867  0.521621  0.690476\n",
       "39             17           0.25          7  0.032939  0.516928  0.690476\n",
       "99             27           0.25          7  0.033096  0.515998  0.690476\n",
       "79             23           0.25          7  0.033050  0.515901  0.690476\n",
       "139            36           0.25          7  0.033157  0.515625  0.690476\n",
       "159            40           0.25          7  0.033195  0.515285  0.690476\n",
       "179            50           0.25          7  0.033195  0.515252  0.690476\n",
       "59             20           0.25          7  0.033041  0.515230  0.690476\n",
       "63             23           0.10          7  0.033149  0.515199  0.690476\n",
       "119            32           0.25          7  0.033195  0.514916  0.690476\n",
       "19             13           0.25          7  0.033120  0.514158  0.690476\n",
       "42             20           0.10          6  0.033662  0.509984  0.690476\n",
       "43             20           0.10          7  0.034452  0.495856  0.690476\n",
       "7              13           0.15          7  0.034616  0.491083  0.690476\n",
       "40             20           0.10          2  0.034897  0.491045  0.690476\n",
       "21             17           0.10          4  0.034848  0.489945  0.690476\n",
       "22             17           0.10          6  0.035702  0.476634  0.690476\n",
       "2              13           0.10          6  0.042012  0.377760  0.690476\n",
       "3              13           0.10          7  0.042650  0.367799  0.690476\n",
       "92             27           0.21          2  0.028071  0.592625  0.666667\n",
       "53             20           0.21          4  0.028161  0.585146  0.666667\n",
       "73             23           0.21          4  0.028238  0.584393  0.666667\n",
       "37             17           0.25          4  0.028247  0.582207  0.666667\n",
       "57             20           0.25          4  0.028355  0.582110  0.666667\n",
       "97             27           0.25          4  0.028282  0.582094  0.666667\n",
       "17             13           0.25          4  0.028463  0.580569  0.666667\n",
       "157            40           0.25          4  0.028838  0.575009  0.666667\n",
       "177            50           0.25          4  0.028939  0.574205  0.666667\n",
       "32             17           0.21          2  0.029546  0.572444  0.666667\n",
       "106            32           0.15          6  0.030050  0.566120  0.666667\n",
       "86             27           0.15          6  0.030205  0.563832  0.666667\n",
       "45             20           0.15          4  0.029745  0.562719  0.666667\n",
       "25             17           0.15          4  0.030587  0.550555  0.666667\n",
       "12             13           0.21          2  0.031155  0.548113  0.666667\n",
       "78             23           0.25          6  0.030891  0.547875  0.666667\n",
       "58             20           0.25          6  0.030927  0.547544  0.666667\n",
       "98             27           0.25          6  0.031042  0.545978  0.666667\n",
       "26             17           0.15          6  0.031495  0.545458  0.666667\n",
       "91             27           0.18          7  0.031193  0.544587  0.666667\n",
       "131            36           0.18          7  0.031215  0.544424  0.666667\n",
       "71             23           0.18          7  0.031256  0.543783  0.666667\n",
       "151            40           0.18          7  0.031269  0.543515  0.666667\n",
       "171            50           0.18          7  0.031370  0.542037  0.666667\n",
       "51             20           0.18          7  0.031461  0.541159  0.666667\n",
       "5              13           0.15          4  0.032879  0.518124  0.666667\n",
       "11             13           0.18          7  0.033454  0.511997  0.666667\n",
       "4              13           0.15          2  0.034581  0.493806  0.666667\n",
       "23             17           0.10          7  0.036860  0.459203  0.666667\n",
       "1              13           0.10          4  0.041466  0.386945  0.666667\n",
       "33             17           0.21          4  0.028116  0.586276  0.642857\n",
       "173            50           0.21          4  0.028328  0.585864  0.642857\n",
       "153            40           0.21          4  0.028339  0.584697  0.642857\n",
       "77             23           0.25          4  0.028351  0.582235  0.642857\n",
       "13             13           0.21          4  0.029017  0.573717  0.642857\n",
       "146            40           0.15          6  0.029882  0.568823  0.642857\n",
       "126            36           0.15          6  0.030006  0.567135  0.642857\n",
       "166            50           0.15          6  0.030011  0.566322  0.642857\n",
       "66             23           0.15          6  0.030331  0.561434  0.642857\n",
       "46             20           0.15          6  0.030653  0.557115  0.642857\n",
       "18             13           0.25          6  0.030908  0.548109  0.642857\n",
       "38             17           0.25          6  0.031003  0.546232  0.642857\n",
       "6              13           0.15          6  0.034013  0.508590  0.642857"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddd6ae0f-609f-49dc-8ffe-806e20a13d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Using a different cross-validation technique for hyperparameter tuning,\n",
    "# #using the run_model function, evaluating for each set of hyperparameters\n",
    "# #and sorting results by accuracy and R2 score\n",
    "\n",
    "\n",
    "# XGB_model = XGBRegressor(n_estimators = 43,\n",
    "#                         learning_rate = 0.16,\n",
    "#                         max_depth = 3\n",
    "#                         )\n",
    "            \n",
    "# model_XGB, mvp_race_lst_XGB, summary_XGB = run_model(XGB_model,\n",
    "#                                                      historical_data_reduced,\n",
    "#                                                      years,\n",
    "#                                                      target='Share'\n",
    "#                                                     )\n",
    "    \n",
    "# mse_XGB = np.mean(summary_XGB['MSE score'].values)\n",
    "# R2_XGB = np.mean(summary_XGB['R2 score'].values)\n",
    "# accuracy_XGB = (summary_XGB['Result'] == 'Right').sum() / len(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbf57562-7438-4369-9452-4e2426b9d0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027595296743234635\n",
      "0.5989780354093357\n",
      "0.7619047619047619\n"
     ]
    }
   ],
   "source": [
    "# print(mse_XGB)\n",
    "# print(R2_XGB)\n",
    "# print(accuracy_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2ba2c-317e-45fe-b216-d2e421443831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
